---
layout: post
title: 【CKA】02-安装、配置、验证
---
- [1. 安装 master、worker(非高可用)](#1-安装-masterworker非高可用)
  - [所有节点都执行的命令](#所有节点都执行的命令)
  - [只有在Master上执行](#只有在master上执行)
  - [只在worker上执行，就一句join](#只在worker上执行就一句join)
  - [最后验证一下。就一句](#最后验证一下就一句)
  - [文字命令](#文字命令)
- [2. 高可用cluster](#2-高可用cluster)
- [3. securing k8s](#3-securing-k8s)
  - [3.1 apiserver接受请求的过程](#31-apiserver接受请求的过程)
    - [首先，apiserver接受请求。做下面四步](#首先apiserver接受请求做下面四步)
    - [最后，把结果写入etcd](#最后把结果写入etcd)
    - [演示](#演示)
  - [3.2 请求发起端的ServiceAccount](#32-请求发起端的serviceaccount)
- [4. 端到端测试，验证pod状态正常](#4-端到端测试验证pod状态正常)
minikube是最好用的单节点k8s

## 1. 安装 master、worker(非高可用)
PS1="node0:~$ "
PS1="node1:~$ "
PS1="node2:~$ "

### 所有节点都执行的命令
![](/docs/images/2020-08-21-10-34-09.png)

### 只有在Master上执行
![](/docs/images/2020-08-21-10-37-28.png)

### 只在worker上执行，就一句join
![](/docs/images/2020-08-21-10-38-58.png)

### 最后验证一下。就一句
![](/docs/images/2020-08-21-10-40-02.png)

### 文字命令
Get the Docker gpg key:
```bash
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
```
Add the Docker repository:
```bash
sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
```
Get the Kubernetes gpg key:
```bash
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
```
Add the Kubernetes repository:
```bash
cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
```
Update your packages:
```bash
sudo apt-get update
```
Install Docker, kubelet, kubeadm, and kubectl:
```bash
sudo apt-get install -y docker-ce=5:19.03.12~3-0~ubuntu-bionic kubelet=1.17.8-00 kubeadm=1.17.8-00 kubectl=1.17.8-00
```
Hold them at the current version:
```bash
sudo apt-mark hold docker-ce kubelet kubeadm kubectl
```
Add the iptables rule to sysctl.conf:
```bash
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
```
Enable iptables immediately:
```bash
sudo sysctl -p
```

Initialize the cluster (run only on the master):
```bash
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
```

Set up local kubeconfig (run only on the master):
```bash
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

Apply Calico CNI network overlay (run only on the master):
```bash
kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml
```
Join the worker nodes to the cluster:

```bash
sudo kubeadm join [your unique string from the kubeadm init command]
```

Verify the worker nodes have joined the cluster successfully:
```bash
kubectl get nodes
```


Compare this result of the kubectl get nodes command:

```bash
NAME                            STATUS   ROLES    AGE   VERSION
chadcrowell1c.mylabserver.com   Ready    master   4m18s v1.17.8
chadcrowell2c.mylabserver.com   Ready    none     82s   v1.17.8
chadcrowell3c.mylabserver.com   Ready    none     69s   v1.17.8
```


## 2. 高可用cluster
![](/docs/images/2020-08-23-08-52-53.png)
![](/docs/images/2020-08-23-08-59-49.png)  

You can provide high availability for cluster components by running multiple instances — however, some replicated components must remain in standby mode. The scheduler and the controller manager are actively watching the cluster state and take action when it changes. If multiples are running, it creates the possibility of unwarranted duplicates of pods.

```bash
[root@centos3 ~]# kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system
POD                                     NODE
kube-proxy-tscrk                        centos1
coredns-bccdc95cf-ss9q9                 centos1
etcd-centos1                            centos1
kube-apiserver-centos1                  centos1
kube-controller-manager-centos1         centos1
kube-scheduler-centos1                  centos1
kube-flannel-ds-amd64-rn7mc             centos1
coredns-bccdc95cf-hsvt6                 centos1
kube-proxy-5lz2q                        centos2
kube-flannel-ds-amd64-xcsg4             centos2
kube-proxy-hjlrp                        centos3
kube-flannel-ds-amd64-8ztst             centos3
kubernetes-dashboard-574b546db9-7frfw   centos3
[root@centos3 ~]#

[root@centos3 ~]# kubectl get endpoints kube-scheduler -n kube-system -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"centos1_4f42572f-05d7-40a6-8963-ecc1876242a5","leaseDurationSeconds":15,"acquireTime":"2020-08-17T05:23:55Z","renewTime":"2020-08-23T00:55:40Z","leaderTransitions":7}'
  creationTimestamp: "2020-06-29T11:23:40Z"
  name: kube-scheduler
  namespace: kube-system
  resourceVersion: "4866299"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler
  uid: a14548f8-b5a9-4440-8a44-b835ffbb678d
[root@centos3 ~]#
```

Create a file called kubeadm-config.yaml
```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
```

Create a stacked etcd topology using kubeadm:
```bash
kubeadm init --config=kubeadm-config.yaml
```

Watch as pods are created in the default namespace:
```bash
kubectl get pods -n kube-system -w
```

**Helpful Links:**  
- [Creating Highly Available Kubernetes Clusters with kubeadm](https://kubernetes.io/docs/setup/independent/high-availability/)  
- [Highly Available Topologies in Kubernetes](https://kubernetes.io/docs/setup/independent/ha-topology/)  
- [Operating a Highly Available etcd Cluster](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/)


## 3. securing k8s
![](/docs/images/2020-08-23-10-03-53.png)

### 3.1 apiserver接受请求的过程

apiserver会接收两种请求，一种是命令行执行的kubectl，还有一种是
pod里执行的请求。

#### 首先，apiserver接受请求。做下面四步
authentiation: 从请求的header中识别出user，group
authorization：判断改user和group是否有权限在改namespace
admission：增改删资源例如pod，需要发送给admission control。
resource validate:
#### 最后，把结果写入etcd

#### 演示
```bash
[root@centos3 kubernetes]# ls
admin.conf  bootstrap-kubelet.conf  kubelet.conf  manifests  pki
[root@centos3 kubernetes]# cat admin.conf
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBdHUwbjJNbz0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://192.168.3.105:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBLQo=
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcEFJQkFBS0NBVkFURSBLRVktLS0tLQo=
```

### 3.2 请求发起端的ServiceAccount
创建pod的时候就会有serviceAccout。并带着一个tokenfile。
例如进入到某个pod的sh，打印出改pod的serviceAccount的token。  
```bash
cat /var/secrets/kubernetes.io/serviceaccount/tocken
```
- pod请求时的serviceaccount
当pod请求apiserver的时候，就是把这个serviceAccount发送给apiserver去提出user，验证user权限。

- kubectl请求时的servcieaccount
默认的serviceaccount.如果不想使用默认service account，必须在manifest目录指定serviceaccount。但是（pod不能指定别的ns的serviceaccount。只能指定自己的ns里的。）
```bash
[root@centos3 kubernetes]# kubectl get serviceaccounts
NAME      SECRETS   AGE
default   1         54d
[root@centos3 kubernetes]#
```
![](/docs/images/2020-08-23-10-12-05.png)
Role-binding：决定了谁能做这些事
role：角色能做哪些事

为了防止未经授权的用户修改群集状态，使用了RBAC，它为用户定义了角色和角色绑定。为Pod创建一个服务帐户资源，以确定它如何控制群集状态。例如，默认服务帐户将不允许您在名称空间中列出服务。

查看kube-config：
```bash
cat .kube/config | more
```
查看服务帐户令牌：
```bash
kubectl get secrets
```
创建一个新的名称空间my-ns：
```bash
kubectl create ns my-ns
```
在my-ns名称空间中运行kube-proxy pod ：
```bash
kubectl run test --image=chadmcrowell/kubectl-proxy -n my-ns
```
列出my-ns命名空间中的Pod ：
```bash
kubectl get pods -n my-ns
```
在新创建的pod中运行shell：
```bash
kubectl exec -it <name-of-pod> -n my-ns sh
```
通过API调用在名称空间中列出服务：
```bash
curl localhost:8001/api/v1/namespaces/my-ns/services
```
从Pod中查看令牌文件：
```bash
cat /var/run/secrets/kubernetes.io/serviceaccount/token
```
列出集群中的服务帐户资源：
```bash
kubectl get serviceaccounts
```

- 有用的网址  
[服务帐号](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/)  
[群集管理概述](http://kubernetes.io/docs/admin)  
[保护集群](https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#securing-a-cluster)  
[控制对API的访问](https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/)  
[授权书](https://kubernetes.io/docs/reference/access-authn-authz/authorization/)  
[使用代理访问API](https://kubernetes.io/docs/tasks/extend-kubernetes/http-proxy-access-api/)


## 4. 端到端测试，验证pod状态正常
运行端到端测试可确保您的应用程序高效运行，而不必担心群集运行状况问题。Kubetest是提供端到端测试的有用工具-但是，它不在本考试范围内。在本课程中，我们将通过实践来测试我们运行部署，运行Pod，暴露容器，从容器执行命令，运行服务以及检查节点和Pod的整体运行状况的能力。

运行一个简单的nginx部署：
```bash
kubectl create deployment nginx --image=nginx
```

查看集群中的部署：
```bash
kubectl get deployments
```
查看集群中的Pod：
```bash
kubectl get pods
```
使用端口转发直接访问Pod：
```bash
kubectl port-forward $pod_name 8081:80
```
直接从Nginx窗格获得响应：
```bash
curl --head http://127.0.0.1:8081
```
从窗格中查看日志：
```bash
kubectl logs $pod_name
```
直接从容器运行命令：
```bash
kubectl exec -it $pod_name -- nginx -v
```
通过暴露nginx部署的端口80创建服务：
```bash
kubectl expose deployment nginx --port 80 --type NodePort
```
列出集群中的服务：
```bash
kubectl get services
```
取得服务的回应：
```bash
curl -I localhost:$node_port
```
列出节点的状态：
```bash
kubectl get nodes
```
查看有关节点的详细信息：
```bash
kubectl describe nodes
```
查看有关吊舱的详细信息：
```bash
kubectl describe pods
```
有用的网址：

[Kubetest](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md)
[测试一个枣簇](https://kubernetes.io/docs/getting-started-guides/ubuntu/)