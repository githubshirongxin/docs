---
layout: post
title: 【spark存在的意义】
---

Hadoop从业者为什么需要Spark？答案是Hadoop已死，Spark称霸。
## 而Hadoop的死亡过程在2012年已经开始：
### 1，由于Hadoop自身架构的导致了在离线数据存储分析以外的一切领域都力不从心，理论已经证明MapReduce模型可以模拟一切分布式计算，但是效率成为其死穴，而Spark基于RDD的计算图可以轻松、完整地表达MapReduce模型，并且能极为容易的处理实时流计算、机器学习、图计算、误差查询等；
### 2,2012年以来Hadoop本身架构臃肿并未得到本质性的改善，很多修改升级也就只是补丁式的修修补补，现在Hadoop这个云计算大数据前期做出卓越贡献的平台正在继续的死亡；
### 4，原先支持Hadoop的四大商业机构纷纷宣布支持Spark；
### 5，Mahout前一阶段表示从现在起他们将不再接受任何形式的以MapReduce形式实现的算法，另外一方面，Mahout宣布新的算法基于Spark；
### 6，Cloudera的机器学习框架Oryx的执行引擎也将由Hadoop的MapReduce替换成Spark；

Spark是继Hadoop之后，成为替代Hadoop的下一代云计算大数据核心技术，目前SPARK已经构建了自己的整个大数据处理生态系统，如流处理、图技术、机器学习、NoSQL查询等方面都有自己的技术，并且是Apache顶级Project，可以预计的是2014年下半年到2015年在社区和商业应用上会有爆发式的增长。
国外一些大型互联网公司已经部署了Spark。甚至连Hadoop的早期主要贡献者Yahoo现在也在多个项目中部署使用Spark；国内的淘宝、优酷土豆、网易、Baidu、腾讯等已经使用Spark技术用于自己的商业生产系统中，国内外的应用开始越来越广泛。Spark正在逐渐走向成熟，并在这个领域扮演更加重要的角色。

现在很多原来使用深度使用Hadoop的公司都在纷纷转向Spark，国内的淘宝是典型的案例，我们在这里就不做介绍。在此，我们以使用世界上使用Hadoop最典型的公司Yahoo！为例，大家可以看一下其数据处理的架构图：
 ![](/docs/images/2021-11-18-16-28-35.png)
而使用Spark后的架构如下：
 ![](/docs/images/2021-11-18-16-28-47.png)

大家可以看出，现阶段的Yahoo！是使用Hadoop和Spark并存的架构，而随着时间的推进和Spark本身流处理、图技术、机器学习、NoSQL查询的出色特性，最终Yahoo！可能会完成Spark全面取代Hadoop，而这也代表了所有做云计算大数据公司的趋势。

最后，不得不提的是Spark的“One stack to rule them all”的特性，Spark的特点之一就是用一个技术堆栈解决云计算大数据中流处理、图技术、机器学习、交互式查询、误差查询等所有的问题，此时我们只需要一个技术团队通过Spark就可以搞定一切问题，而如果基于Hadoop就需要分别构建实时流处理团队、数据统计分析团队、数据挖掘团队等，而且这些团队之间无论是代码还是经验都不可相互借鉴，会形成巨大的成本，而使用Spark就不存在这个问题；
再说一点，Hadoop现在人才已经非常多了，想在该技术领域做到中国前100人之内是非常有难度的，而如果从事Spark则会更容易些，因为现在Spark人才不是稀少，而是太稀缺。

Hadoop从业者们，您需要Spark。

